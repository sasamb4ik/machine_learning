{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4c90083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ (1) y = \\langle w, x \\rangle$$\n",
       "$$ (2) Y = Xw $$\n",
       "$$ (3) w = (X^{T}X)^{-1}X^{T}Y $$\n",
       "# (1) - запись линейной регрессии в виде скалярного произведения (с наличием байеса (то есть свободного)  члена w0, к которому мы добавили фиктивный признак, равный единице, чтобы удобно было представить в виде скалярного произведения). (2) запись лин рег в матричном виде, где X - матрица (вектор) признаков размера n на k, w - вектор весов размера k. (3) аналитическое решение для минимализации ошибки MSE для матричной формы\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# линейная регрессия, Lasso и Ridge регрессии и методы оптимизации\n",
    "%%latex\n",
    "$$ (1) y = \\langle w, x \\rangle$$\n",
    "$$ (2) Y = Xw $$\n",
    "$$ (3) w = (X^{T}X)^{-1}X^{T}Y $$\n",
    "# (1) - запись линейной регрессии в виде скалярного произведения (с наличием байеса (то есть свободного)  члена w0, к которому мы добавили фиктивный признак, равный единице, чтобы удобно было представить в виде скалярного произведения). (2) запись лин рег в матричном виде, где X - матрица (вектор) признаков размера n на k, w - вектор весов размера k. (3) аналитическое решение для минимализации ошибки MSE для матричной формы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15bf60fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.83186993, 5.0042125 ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) АНАЛИТИЧЕСКОЕ РЕШЕНИЕ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class MyLinearRegression:\n",
    "    def __init__(self, fit_intercept = True):\n",
    "        self.fit_intercept = fit_intercept # fit_intercept отвечает за наличие байеса \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n, k = X.shape\n",
    "        X_train = X\n",
    "        if self.fit_intercept:\n",
    "            X_train = np.hstack((X, np.ones((n, 1)))) # если есть байес, то добавляем фиктивную 1 \n",
    "                                                      # для удобного вида в скалярном произведении\n",
    "        \n",
    "        self.w = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        n, k = X.shape\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X_train = np.hstack((X, np.ones((n, 1))))\n",
    "            \n",
    "        y_pred = X_train @ self.w\n",
    "        return y_pred\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.w\n",
    "    \n",
    "def linear_expression(x):\n",
    "    return 5 * x + 6\n",
    "\n",
    "\n",
    "# недостатки аналитического решения: 1) посчитать inv обратную матрицу долго - куб операций\n",
    "#                                    2) обратная матрица не всегда есть\n",
    "# обратной матрицы может не быть, если мы имеем дело с вырожденной матрицей (в ней есть линейно \n",
    "# зависимые строки или столбцы). поэтому используется более серьезный метод - Градиентная оптимизация\n",
    "# (градиентный спуск)\n",
    "\n",
    "\n",
    "#стандартный (медленный вариант градиента)\n",
    "\n",
    "class MyGradientLinearRegression(MyLinearRegression):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.w = None # итоговые веса\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, shag = 0.01,  max_iter = 100):\n",
    "        n, k = X.shape\n",
    "        # случайно зададим веса\n",
    "        if self.w is None:\n",
    "            self.w = np.random.randn(k + 1 if self.fit_intercept else k)\n",
    "        \n",
    "        X_train = np.hstack((X, np.ones((n, 1)))) if self.fit_intercept else X\n",
    "        \n",
    "        self.losses = []\n",
    "        \n",
    "        for iter_num in range(max_iter):\n",
    "            y_pred = self.predict(X)\n",
    "            self.losses.append(mean_squared_error(y_pred, y))\n",
    "            \n",
    "            grad = self.calc_gradient(X_train, y)\n",
    "            \n",
    "            self.w -= shag * grad\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        n, k = X.shape\n",
    "        grad = (2 / n) * np.dot(X.T, (np.dot(X, self.w) - y))\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    def get_losses(self):\n",
    "        return self.losses\n",
    "    \n",
    "\n",
    "# стохастический mini-batch. главное его отличие - это использование подвыборки для шага оптимизации\n",
    "# (а не всей выборки, как в обычном спуске)\n",
    "\n",
    "class StochasticMiniBatch(MyGradientLinearRegression):\n",
    "    def __init__(self, n_sample = 10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.w = None # итоговые веса\n",
    "        self.n_sample = n_sample # кол-во элементов для подвыборки\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        n, k = X.shape \n",
    "        inds = np.random.choice(np.arange(X.shape[0]), size = self.n_sample, replace = False)\n",
    "        grad = (2 / n) * (np.dot(X.T, np.dot(X, self.w) - y))\n",
    "        return grad\n",
    "\n",
    "regressor = StochasticMiniBatch(fit_intercept=True)\n",
    "\n",
    "l = regressor.fit(X_train[:, np.newaxis], y_train, max_iter=100).get_losses()\n",
    "\n",
    "predictions = regressor.predict(X_test[:, np.newaxis])\n",
    "w = regressor.get_weights()\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97eb0057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\nabla Q(w) = \\frac{2}{l}X^{T}(X_w - y)$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex #формула градиентного спуска в матричной форме:\n",
    "$$\\nabla Q(w) = \\frac{2}{l}X^{T}(X_w - y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6136b7f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$w^{t+1} = w^t - a_t\\nabla Q(w^{t})$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex \n",
    "$$w^{t+1} = w^t - a_t\\nabla Q(w^{t})$$ # правило обновления весов в градиентном спуске"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2d9f9c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$w^{t} - w^{t-1} < e$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex # ФОРМУЛА(2) # проверка для остановки в градиенте\n",
    "$$w^{t} - w^{t-1} < e$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bb2e574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$Q(w) = \\frac{1}{l}\\sum_{i=1}^l(\\langle w, x\\rangle - y_i)$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$$Q(w) = \\frac{1}{l}\\sum_{i=1}^l(\\langle w, x\\rangle - y_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# линейная регрессия в матричном (векторном виде)\n",
    "# матричная форма намного быстрее обычного градиента\n",
    "# всё, что меняется - это формула градиента в матричной форме\n",
    "class LinearRegressionVectorized(BaseEstimator): \n",
    "\n",
    "    def __init__(self, epsilon=1e-4, w0 = None, max_steps = 1000, alpha=1e-2):\n",
    "        self.epsilon = epsilon\n",
    "        self.w0 = w0\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.w_history = []\n",
    "    def fit(self, X, y):\n",
    "        l, d = X.shape\n",
    "        if self.w0 == None:\n",
    "            self.w0 = np.zeros(d)\n",
    "        self.w = w0\n",
    "        for step in range(self.max_steps):\n",
    "            self.w_history.append(self.w)\n",
    "            w_new = self.w - self.alpha * self.calc_gradient(X, y)\n",
    "            if np.linalg.norm(self.w_new - self.w) < epsilon:\n",
    "                break\n",
    "        self.w = self.w_new\n",
    "        return self\n",
    "    \n",
    "    def calc_gradient_vectorized(self, X, y):\n",
    "        l, d = X.shape\n",
    "        return (2/l)*np.dot(X.T, (np.dot(X, self.w) - y))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if self.w == None:\n",
    "            raise Exception 'Модель не обучена'\n",
    "        return np.dot(X, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29214fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## далее про линейный классификатор (обучение с градиентным спуском)\n",
    "# функция потерь линейного классификатора - формула(1)\n",
    "# градиент функции потерь линейного классификатора - формула(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7195fbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$(1) Q(w,x) = \\frac{1}{l}\\sum_{i=1}^llog(1 + exp(-y_i\\langle w, x_i\\rangle))$$\n",
       "$$(2) \\nabla Q(w,x) = -\\frac{1}{l}\\sum_{i = 1}^l\\frac{y_ix_i}{1 + exp(y_i\\langle w,x_i \\rangle)}$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$$(1) Q(w,x) = \\frac{1}{l}\\sum_{i=1}^llog(1 + exp(-y_i\\langle w, x_i\\rangle))$$\n",
    "$$(2) \\nabla Q(w,x) = -\\frac{1}{l}\\sum_{i = 1}^l\\frac{y_ix_i}{1 + exp(y_i\\langle w,x_i \\rangle)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a92084d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 500 is different from 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22976/1980235840.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# from sklearn.linear_model import SGDClassifier - готовая реазилация с градиетным спуском\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22976/1980235840.py\u001b[0m in \u001b[0;36mgradient_descent_classifier\u001b[1;34m(X, y, w_init, n_steps, eta)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgradient_descent_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_init\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mloss_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mw_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22976/1980235840.py\u001b[0m in \u001b[0;36mlog_loss\u001b[1;34m(w, X, y)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 500 is different from 3)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_loss(w, X, y):\n",
    "    Q = np.log(1 + np.exp(-y * (w @ X))).mean()\n",
    "    return Q\n",
    "\n",
    "def calc_grad(w, X, y):\n",
    "    Q_grad = -((X * y.reshape(-1, 1)) / (1 + np.exp(y * (w @ X))).reshape(-1, 1)).mean(axis = 0)\n",
    "    return Q_grad\n",
    "\n",
    "w_init = np.array([1.0, 1.0, 1.0])\n",
    "X_new = np.c_[np.ones(n), X]\n",
    "\n",
    "def gradient_descent_classifier(X, y, w_init, n_steps, eta):\n",
    "    w = w_init.copy()\n",
    "    loss_array = [log_loss(w_init, X, y)]\n",
    "    for step in range(n_steps):\n",
    "        w_grad = calc_grad(w_init, X, y)\n",
    "        w -= eta * w_grad\n",
    "        loss = log_loss(w, X, y)\n",
    "        loss_array.append(loss)\n",
    "    return loss_array, w\n",
    "\n",
    "w, loss_array = gradient_descent_classifier(X_new, y, w_init, n_steps=1000, eta=0.1)\n",
    "\n",
    "# from sklearn.linear_model import SGDClassifier - готовая реазилация с градиетным спуском\n",
    "\n",
    "\n",
    "## вероятность классификация \n",
    "# модель вместо класса -1 или 1 возвращает вероятность принадлежности к определенному классу \n",
    "# чтобы из такого предсказания в итоге получить класс, нужно задать некоторый порог для вероятности\n",
    "\n",
    "\n",
    "def predict_probabilities(X, w):\n",
    "    return 1 / (1 + exp(-X @ w))\n",
    "\n",
    "y_pred = predict_probabilities(X_new, w)\n",
    "\n",
    "def check(y_pred, t):\n",
    "    y_pred_classes = y_pred.copy()\n",
    "    y_pred_classes[y_pred_classes > t] = 1\n",
    "    y_pred_classes[y_pred_classes <= t] = -1\n",
    "    return y_pred_classes\n",
    "\n",
    "t = 0.5 # порог\n",
    "# в зависимости от порога будут меняться Precision и Recall. найти оптимальный порог можно по \n",
    "# PR - кривой (Precision-Recall Curve)\n",
    "\n",
    "#from sklearn.metrics import precision_recall_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ffb2e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.613107822410148"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "data = pd.read_csv('heart.csv')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis = 1), data['target'], test_size=0.25, random_state=13)\n",
    "\n",
    "clf = SGDClassifier(max_iter = 1000, learning_rate='constant', eta0 = 0.1, alpha = 0, loss = 'log', random_state = 13)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.coef_ # смотрим полученные веса (без свободного кф-а)\n",
    "clf.intercept_ # смотрим свободный коэффицент\n",
    "np.linalg.norm(clf.coef_) # считаем L-2 норму \n",
    "ans = clf.predict(X_test)\n",
    "score = precision_score(y_test, ans)\n",
    "score\n",
    "\n",
    "# теперь попробуем применить регуляризацию\n",
    "# за регуляризацию (штраф ща большие веса, это препятствует переобучению модели)\n",
    "# L1 - лассо регуляризация ; L2 - гребневая регуляризация\n",
    "# в sklearn.linear_model.SGDClassifier - параметр регуляризации обозначается параметром alpha.\n",
    "# За тип регуляризации (L1, L2 или обе сразу) отвечает параметр penalty.\n",
    "\n",
    "clf_reg = SGDClassifier(max_iter=1000, random_state = 13, penalty = 'l1', alpha = 0.1, learning_rate = 'optimal')\n",
    "clf_reg.fit(X_train, y_train)\n",
    "ans_2 = clf_reg.predict(X_test)\n",
    "score_2 = precision_score(y_test, ans_2)\n",
    "np.linalg.norm(clf_reg.coef_)\n",
    "\n",
    "\n",
    "res_2 = roc_auc_score(y_test, ans_2)\n",
    "res_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb878b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46786f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c39bbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8733a65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcaa0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03bba80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65126515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e295d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6134da3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1c0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "90px",
    "width": "214px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
