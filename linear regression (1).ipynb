{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4c90083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ (1) y = \\langle w, x \\rangle$$\n",
       "$$ (2) Y = Xw $$\n",
       "$$ (3) w = (X^{T}X)^{-1}X^{T}Y $$\n",
       "# (1) - запись линейной регрессии в виде скалярного произведения (с наличием байеса (то есть свободного)  члена w0, к которому мы добавили фиктивный признак, равный единице, чтобы удобно было представить в виде скалярного произведения). (2) запись лин рег в матричном виде, где X - матрица (вектор) признаков размера n на k, w - вектор весов размера k. (3) аналитическое решение для минимализации ошибки MSE для матричной формы\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# линейная регрессия, Lasso и Ridge регрессии и методы оптимизации\n",
    "%%latex\n",
    "$$ (1) y = \\langle w, x \\rangle$$\n",
    "$$ (2) Y = Xw $$\n",
    "$$ (3) w = (X^{T}X)^{-1}X^{T}Y $$\n",
    "# (1) - запись линейной регрессии в виде скалярного произведения (с наличием байеса (то есть свободного)  члена w0, к которому мы добавили фиктивный признак, равный единице, чтобы удобно было представить в виде скалярного произведения). (2) запись лин рег в матричном виде, где X - матрица (вектор) признаков размера n на k, w - вектор весов размера k. (3) аналитическое решение для минимализации ошибки MSE для матричной формы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15bf60fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.83186993, 5.0042125 ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) АНАЛИТИЧЕСКОЕ РЕШЕНИЕ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class MyLinearRegression:\n",
    "    def __init__(self, fit_intercept = True):\n",
    "        self.fit_intercept = fit_intercept # fit_intercept отвечает за наличие байеса \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n, k = X.shape\n",
    "        X_train = X\n",
    "        if self.fit_intercept:\n",
    "            X_train = np.hstack((X, np.ones((n, 1)))) # если есть байес, то добавляем фиктивную 1 \n",
    "                                                      # для удобного вида в скалярном произведении\n",
    "        \n",
    "        self.w = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        n, k = X.shape\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X_train = np.hstack((X, np.ones((n, 1))))\n",
    "            \n",
    "        y_pred = X_train @ self.w\n",
    "        return y_pred\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.w\n",
    "    \n",
    "def linear_expression(x):\n",
    "    return 5 * x + 6\n",
    "\n",
    "\n",
    "# недостатки аналитического решения: 1) посчитать inv обратную матрицу долго - куб операций\n",
    "#                                    2) обратная матрица не всегда есть\n",
    "# обратной матрицы может не быть, если мы имеем дело с вырожденной матрицей (в ней есть линейно \n",
    "# зависимые строки или столбцы). поэтому используется более серьезный метод - Градиентная оптимизация\n",
    "# (градиентный спуск)\n",
    "\n",
    "\n",
    "#стандартный (медленный вариант градиента)\n",
    "\n",
    "class MyGradientLinearRegression(MyLinearRegression):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.w = None # итоговые веса\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, shag = 0.01,  max_iter = 100):\n",
    "        n, k = X.shape\n",
    "        # случайно зададим веса\n",
    "        if self.w is None:\n",
    "            self.w = np.random.randn(k + 1 if self.fit_intercept else k)\n",
    "        \n",
    "        X_train = np.hstack((X, np.ones((n, 1)))) if self.fit_intercept else X\n",
    "        \n",
    "        self.losses = []\n",
    "        \n",
    "        for iter_num in range(max_iter):\n",
    "            y_pred = self.predict(X)\n",
    "            self.losses.append(mean_squared_error(y_pred, y))\n",
    "            \n",
    "            grad = self.calc_gradient(X_train, y)\n",
    "            \n",
    "            self.w -= shag * grad\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        n, k = X.shape\n",
    "        grad = (2 / n) * np.dot(X.T, (np.dot(X, self.w) - y))\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    def get_losses(self):\n",
    "        return self.losses\n",
    "    \n",
    "\n",
    "# стохастический mini-batch. главное его отличие - это использование подвыборки для шага оптимизации\n",
    "# (а не всей выборки, как в обычном спуске)\n",
    "\n",
    "class StochasticMiniBatch(MyGradientLinearRegression):\n",
    "    def __init__(self, n_sample = 10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.w = None # итоговые веса\n",
    "        self.n_sample = n_sample # кол-во элементов для подвыборки\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        n, k = X.shape \n",
    "        inds = np.random.choice(np.arange(X.shape[0]), size = self.n_sample, replace = False)\n",
    "        grad = (2 / n) * (np.dot(X.T, np.dot(X, self.w) - y))\n",
    "        return grad\n",
    "\n",
    "regressor = StochasticMiniBatch(fit_intercept=True)\n",
    "\n",
    "l = regressor.fit(X_train[:, np.newaxis], y_train, max_iter=100).get_losses()\n",
    "\n",
    "predictions = regressor.predict(X_test[:, np.newaxis])\n",
    "w = regressor.get_weights()\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97eb0057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\nabla Q(w) = \\frac{2}{l}X^{T}(X_w - y)$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex #формула градиентного спуска в матричной форме:\n",
    "$$\\nabla Q(w) = \\frac{2}{l}X^{T}(X_w - y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6136b7f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$w^{t+1} = w^t - a_t\\nabla Q(w^{t})$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex \n",
    "$$w^{t+1} = w^t - a_t\\nabla Q(w^{t})$$ # правило обновления весов в градиентном спуске"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2d9f9c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$w^{t} - w^{t-1} < e$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex # ФОРМУЛА(2) # проверка для остановки в градиенте\n",
    "$$w^{t} - w^{t-1} < e$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bb2e574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$Q(w) = \\frac{1}{l}\\sum_{i=1}^l(\\langle w, x\\rangle - y_i)$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$$Q(w) = \\frac{1}{l}\\sum_{i=1}^l(\\langle w, x\\rangle - y_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# линейная регрессия в матричном (векторном виде)\n",
    "# матричная форма намного быстрее обычного градиента\n",
    "# всё, что меняется - это формула градиента в матричной форме\n",
    "class LinearRegressionVectorized(BaseEstimator): \n",
    "\n",
    "    def __init__(self, epsilon=1e-4, w0 = None, max_steps = 1000, alpha=1e-2):\n",
    "        self.epsilon = epsilon\n",
    "        self.w0 = w0\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.w_history = []\n",
    "    def fit(self, X, y):\n",
    "        l, d = X.shape\n",
    "        if self.w0 == None:\n",
    "            self.w0 = np.zeros(d)\n",
    "        self.w = w0\n",
    "        for step in range(self.max_steps):\n",
    "            self.w_history.append(self.w)\n",
    "            w_new = self.w - self.alpha * self.calc_gradient(X, y)\n",
    "            if np.linalg.norm(self.w_new - self.w) < epsilon:\n",
    "                break\n",
    "        self.w = self.w_new\n",
    "        return self\n",
    "    \n",
    "    def calc_gradient_vectorized(self, X, y):\n",
    "        l, d = X.shape\n",
    "        return (2/l)*np.dot(X.T, (np.dot(X, self.w) - y))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if self.w == None:\n",
    "            raise Exception 'Модель не обучена'\n",
    "        return np.dot(X, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29214fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195fbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a92084d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5df44b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb878b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46786f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c39bbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8733a65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcaa0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03bba80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65126515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e295d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6134da3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1c0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
